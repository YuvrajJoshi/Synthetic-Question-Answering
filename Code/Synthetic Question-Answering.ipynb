{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Question Answering\n",
    "Work on Babi Dataset from Facebook Research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_qa.txt', \"rb\") as fp:    #Unpickling\n",
    "    train_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_qa.txt', \"rb\") as fp:     #Unpickling\n",
    "    test_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Format of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train type: <class 'list'>\n",
      "Test type: <class 'list'>\n",
      "Train length: 10000\n",
      "Test length: 1000\n"
     ]
    }
   ],
   "source": [
    "train_type = type(train_data)\n",
    "print(f\"Train type: {train_type}\")\n",
    "test_type = type(test_data)\n",
    "print(f\"Test type: {test_type}\")\n",
    "train_len = len(train_data)\n",
    "print(f\"Train length: {train_len}\")\n",
    "test_len = len(test_data)\n",
    "print(f\"Test length: {test_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Sandra in the hallway ?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that our data has some Stories/Statements, based on those we have some Questions and an Answer to those questions in \"yes\" or \"no.\" \n",
    "We will call our stories/statements as a set of inputs 'X', questions as query 'q' and the output/answer to those as 'a'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Vocabulary of All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "full_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question, answer in full_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_story_len = max([len(data[0]) for data in full_data])\n",
    "max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_query_len = max([len(data[1]) for data in full_data])\n",
    "max_query_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add an extra space to hold a 0 for Keras' pad sequences\n",
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mary': 1, 'sandra': 2, '?': 3, 'bathroom': 4, 'office': 5, 'picked': 6, 'moved': 7, '.': 8, 'went': 9, 'journeyed': 10, 'hallway': 11, 'is': 12, 'apple': 13, 'yes': 14, 'put': 15, 'in': 16, 'daniel': 17, 'discarded': 18, 'milk': 19, 'took': 20, 'kitchen': 21, 'down': 22, 'garden': 23, 'travelled': 24, 'back': 25, 'dropped': 26, 'john': 27, 'bedroom': 28, 'to': 29, 'the': 30, 'there': 31, 'got': 32, 'no': 33, 'grabbed': 34, 'left': 35, 'up': 36, 'football': 37}\n"
     ]
    }
   ],
   "source": [
    "#tokens assigned to each unique from our vocab(vocabulary)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_query_len=max_query_len):\n",
    "    '''\n",
    "    INPUT:\n",
    "    \n",
    "    data: our data consisting Statements, Questions and Answers\n",
    "    word_index: word index dictionary from tokenizer (which we created in the tokenization phase)\n",
    "    max_story_len: the length of the longest group of input statements\n",
    "    maax_question_len: length of longest question\n",
    "    \n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    returns a tuple (X,Xq,Y) of padded sequences.\n",
    "    '''\n",
    "    #Stories\n",
    "    X = []\n",
    "    #Query/Questions\n",
    "    Xq = []\n",
    "    #Correct Answer\n",
    "    Y = []\n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        #Grab the word index for every word in story\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        #Grab the word index for every word in query\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        #Now grab the answers, index 0 is reserved so +1\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        #Use numpy logic to create this assignment\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        #Append the gathered values to their respective lists\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    \n",
    "    #Finally, pad the sequences based on their max length so that RNN can be trained on uniformly long sequences\n",
    "    \n",
    "    return (pad_sequences(X, maxlen=max_story_len), pad_sequences(Xq, maxlen=max_query_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_queries, train_answers = vectorize_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_queries, test_answers = vectorize_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For train and test both we will have as depicted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 30, 28,  8],\n",
       "       [ 0,  0,  0, ..., 30, 11,  8],\n",
       "       [ 0,  0,  0, ..., 30,  4,  8],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 30, 28,  8],\n",
       "       [ 0,  0,  0, ..., 19, 31,  8],\n",
       "       [ 0,  0,  0, ..., 13, 31,  8]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs are an array of lists having padded sequences containing the word indexes corresponding to the word in that particular story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  2, 16, 30, 11,  3],\n",
       "       [12, 17, 16, 30,  4,  3],\n",
       "       [12, 17, 16, 30,  5,  3],\n",
       "       ...,\n",
       "       [12,  2, 16, 30, 11,  3],\n",
       "       [12,  1, 16, 30, 21,  3],\n",
       "       [12,  1, 16, 30, 28,  3]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries will be arrays of list containing for each item of the array i.e a query, the word indexes of the words present in that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally answers will also be an array of list where either the yes or no word_index will be marked according to the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Placeholders for Inputs\n",
    "Since we have two inputs (stories and questions), so we need to use placeholders. `Input()` is used to instantiate a Keras tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholder shape=(max_story_len, batch_size) \n",
    "#We haven't defined the batch size yet.\n",
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_query_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Encoders\n",
    "*Input Encoder m*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input gets embedded to a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=256))\n",
    "input_encoder_m.add(Dropout(0.4))\n",
    "# This encoder will output:\n",
    "# (samples, story_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input Encoder c*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=max_query_len))\n",
    "input_encoder_c.add(Dropout(0.4))\n",
    "# output: (samples, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question Encoder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,output_dim=256,input_length=max_query_len))\n",
    "question_encoder.add(Dropout(0.4))\n",
    "# output: (samples, query_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the match between first input vector seq and the query\n",
    "match = dot([input_encoded_m,question_encoded], axes=(2,2))\n",
    "match = Activation('softmax')(match)\n",
    "#match is a probability vector over the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the match matrix with second input vector sequence\n",
    "response = add([match, input_encoded_c])\n",
    "response = Permute((2,1))(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce with RNN (LSTM)\n",
    "answer = LSTM(32)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing regularization again\n",
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_size)(answer)     # (samples, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "#build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      multiple             9728        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 6, 256)       9728        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 156, 6)       0           sequential_10[1][0]              \n",
      "                                                                 sequential_12[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 156, 6)       0           dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      multiple             228         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 156, 6)       0           activation_7[0][0]               \n",
      "                                                                 sequential_11[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 6, 156)       0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 6, 412)       0           permute_4[0][0]                  \n",
      "                                                                 sequential_12[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 32)           56960       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 32)           0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 38)           1254        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 38)           0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 77,898\n",
      "Trainable params: 77,898\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 9s 933us/step - loss: 0.9197 - accuracy: 0.4987 - val_loss: 0.6954 - val_accuracy: 0.4970\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 8s 805us/step - loss: 0.7030 - accuracy: 0.4929 - val_loss: 0.6944 - val_accuracy: 0.4970\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 8s 823us/step - loss: 0.6954 - accuracy: 0.5022 - val_loss: 0.6934 - val_accuracy: 0.5030\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 10s 981us/step - loss: 0.6956 - accuracy: 0.4921 - val_loss: 0.6935 - val_accuracy: 0.4970\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 10s 986us/step - loss: 0.6945 - accuracy: 0.4976 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 9s 886us/step - loss: 0.6942 - accuracy: 0.5030 - val_loss: 0.6987 - val_accuracy: 0.4970\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 9s 877us/step - loss: 0.6942 - accuracy: 0.4991 - val_loss: 0.6973 - val_accuracy: 0.4980\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 9s 925us/step - loss: 0.6922 - accuracy: 0.5153 - val_loss: 0.6897 - val_accuracy: 0.5260\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 9s 881us/step - loss: 0.6264 - accuracy: 0.6668 - val_loss: 0.5422 - val_accuracy: 0.7250\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.4676 - accuracy: 0.8034 - val_loss: 0.4223 - val_accuracy: 0.8320\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 0.4131 - accuracy: 0.8292 - val_loss: 0.4105 - val_accuracy: 0.8210\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.3881 - accuracy: 0.8417 - val_loss: 0.4284 - val_accuracy: 0.8370\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 9s 861us/step - loss: 0.3731 - accuracy: 0.8489 - val_loss: 0.3985 - val_accuracy: 0.8390\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.3643 - accuracy: 0.8501 - val_loss: 0.3791 - val_accuracy: 0.8330\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.3522 - accuracy: 0.8544 - val_loss: 0.4026 - val_accuracy: 0.8100\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 9s 864us/step - loss: 0.3437 - accuracy: 0.8554 - val_loss: 0.3690 - val_accuracy: 0.8330\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 9s 861us/step - loss: 0.3288 - accuracy: 0.8610 - val_loss: 0.3689 - val_accuracy: 0.8350\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 9s 869us/step - loss: 0.3281 - accuracy: 0.8604 - val_loss: 0.3689 - val_accuracy: 0.8230\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 9s 861us/step - loss: 0.3192 - accuracy: 0.8630 - val_loss: 0.3775 - val_accuracy: 0.8370\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 9s 858us/step - loss: 0.3178 - accuracy: 0.8627 - val_loss: 0.3526 - val_accuracy: 0.8320\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 9s 858us/step - loss: 0.3113 - accuracy: 0.8635 - val_loss: 0.3473 - val_accuracy: 0.8410\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.3098 - accuracy: 0.8652 - val_loss: 0.3471 - val_accuracy: 0.8300\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 9s 861us/step - loss: 0.3059 - accuracy: 0.8683 - val_loss: 0.3428 - val_accuracy: 0.8390\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 9s 895us/step - loss: 0.3016 - accuracy: 0.8723 - val_loss: 0.3433 - val_accuracy: 0.8370\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.3043 - accuracy: 0.8686 - val_loss: 0.3502 - val_accuracy: 0.8350\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 9s 859us/step - loss: 0.2982 - accuracy: 0.8704 - val_loss: 0.3473 - val_accuracy: 0.8360\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 9s 853us/step - loss: 0.2990 - accuracy: 0.8655 - val_loss: 0.3538 - val_accuracy: 0.8370\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.2925 - accuracy: 0.8692 - val_loss: 0.3634 - val_accuracy: 0.8370\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 9s 878us/step - loss: 0.2959 - accuracy: 0.8699 - val_loss: 0.3446 - val_accuracy: 0.8340\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 9s 861us/step - loss: 0.2958 - accuracy: 0.8693 - val_loss: 0.3379 - val_accuracy: 0.8280\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 9s 856us/step - loss: 0.2946 - accuracy: 0.8700 - val_loss: 0.3441 - val_accuracy: 0.8290\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 9s 867us/step - loss: 0.2908 - accuracy: 0.8735 - val_loss: 0.3307 - val_accuracy: 0.8360\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 9s 858us/step - loss: 0.2898 - accuracy: 0.8749 - val_loss: 0.3452 - val_accuracy: 0.8360\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.2892 - accuracy: 0.8766 - val_loss: 0.3441 - val_accuracy: 0.8400\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.2899 - accuracy: 0.8709 - val_loss: 0.3484 - val_accuracy: 0.8370\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.2887 - accuracy: 0.8763 - val_loss: 0.3462 - val_accuracy: 0.8370\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 9s 870us/step - loss: 0.2849 - accuracy: 0.8780 - val_loss: 0.3645 - val_accuracy: 0.8250\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.2857 - accuracy: 0.8773 - val_loss: 0.3338 - val_accuracy: 0.8430\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.2877 - accuracy: 0.8751 - val_loss: 0.3487 - val_accuracy: 0.8400\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.2840 - accuracy: 0.8768 - val_loss: 0.3877 - val_accuracy: 0.8410\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.2840 - accuracy: 0.8803 - val_loss: 0.3390 - val_accuracy: 0.8390\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 9s 911us/step - loss: 0.2817 - accuracy: 0.8831 - val_loss: 0.3590 - val_accuracy: 0.8400\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 9s 903us/step - loss: 0.2813 - accuracy: 0.8796 - val_loss: 0.3629 - val_accuracy: 0.8420\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 9s 914us/step - loss: 0.2810 - accuracy: 0.8803 - val_loss: 0.3482 - val_accuracy: 0.8380\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 9s 875us/step - loss: 0.2806 - accuracy: 0.8827 - val_loss: 0.3473 - val_accuracy: 0.8360\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 9s 919us/step - loss: 0.2796 - accuracy: 0.8789 - val_loss: 0.3572 - val_accuracy: 0.8420\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 9s 883us/step - loss: 0.2767 - accuracy: 0.8827 - val_loss: 0.3594 - val_accuracy: 0.8490\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 0.2741 - accuracy: 0.8828 - val_loss: 0.3440 - val_accuracy: 0.8400\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 9s 880us/step - loss: 0.2769 - accuracy: 0.8823 - val_loss: 0.3816 - val_accuracy: 0.8420\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 9s 870us/step - loss: 0.2682 - accuracy: 0.8839 - val_loss: 0.3492 - val_accuracy: 0.8370\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 0.2711 - accuracy: 0.8867 - val_loss: 0.3611 - val_accuracy: 0.8360\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 9s 923us/step - loss: 0.2702 - accuracy: 0.8877 - val_loss: 0.3583 - val_accuracy: 0.8470\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.2662 - accuracy: 0.8897 - val_loss: 0.3466 - val_accuracy: 0.8430\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 9s 923us/step - loss: 0.2665 - accuracy: 0.8882 - val_loss: 0.3558 - val_accuracy: 0.8430\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 9s 865us/step - loss: 0.2642 - accuracy: 0.8917 - val_loss: 0.3553 - val_accuracy: 0.8500\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 9s 880us/step - loss: 0.2670 - accuracy: 0.8901 - val_loss: 0.3475 - val_accuracy: 0.8510\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 8s 841us/step - loss: 0.2605 - accuracy: 0.8965 - val_loss: 0.3892 - val_accuracy: 0.8400\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 8s 837us/step - loss: 0.2590 - accuracy: 0.8944 - val_loss: 0.3558 - val_accuracy: 0.8510\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 8s 847us/step - loss: 0.2588 - accuracy: 0.8963 - val_loss: 0.3586 - val_accuracy: 0.8480\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 9s 889us/step - loss: 0.2524 - accuracy: 0.8982 - val_loss: 0.3521 - val_accuracy: 0.8500\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 9s 903us/step - loss: 0.2523 - accuracy: 0.8985 - val_loss: 0.3474 - val_accuracy: 0.8530\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 9s 919us/step - loss: 0.2507 - accuracy: 0.8998 - val_loss: 0.3651 - val_accuracy: 0.8590\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.2506 - accuracy: 0.9019 - val_loss: 0.3550 - val_accuracy: 0.8510\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 8s 841us/step - loss: 0.2471 - accuracy: 0.9026 - val_loss: 0.3565 - val_accuracy: 0.8520\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 8s 843us/step - loss: 0.2464 - accuracy: 0.8998 - val_loss: 0.3559 - val_accuracy: 0.8460\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 9s 936us/step - loss: 0.2444 - accuracy: 0.9056 - val_loss: 0.3910 - val_accuracy: 0.8510\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.2476 - accuracy: 0.9032 - val_loss: 0.3572 - val_accuracy: 0.8560\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 8s 836us/step - loss: 0.2360 - accuracy: 0.9049 - val_loss: 0.3564 - val_accuracy: 0.8560\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 8s 845us/step - loss: 0.2439 - accuracy: 0.9034 - val_loss: 0.3588 - val_accuracy: 0.8540\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 9s 852us/step - loss: 0.2336 - accuracy: 0.9072 - val_loss: 0.3514 - val_accuracy: 0.8520\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.2363 - accuracy: 0.9073 - val_loss: 0.3615 - val_accuracy: 0.8520\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 9s 856us/step - loss: 0.2320 - accuracy: 0.9103 - val_loss: 0.3551 - val_accuracy: 0.8590\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 9s 891us/step - loss: 0.2300 - accuracy: 0.9080 - val_loss: 0.4210 - val_accuracy: 0.8470\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 8s 845us/step - loss: 0.2310 - accuracy: 0.9062 - val_loss: 0.3578 - val_accuracy: 0.8600\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 9s 850us/step - loss: 0.2244 - accuracy: 0.9089 - val_loss: 0.3330 - val_accuracy: 0.8500\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 9s 938us/step - loss: 0.2244 - accuracy: 0.9107 - val_loss: 0.3671 - val_accuracy: 0.8520\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 0.2273 - accuracy: 0.9072 - val_loss: 0.3556 - val_accuracy: 0.8640\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 9s 881us/step - loss: 0.2197 - accuracy: 0.9122 - val_loss: 0.3327 - val_accuracy: 0.8640\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 9s 869us/step - loss: 0.2169 - accuracy: 0.9132 - val_loss: 0.3375 - val_accuracy: 0.8600\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.2125 - accuracy: 0.9121 - val_loss: 0.3372 - val_accuracy: 0.8640\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.2095 - accuracy: 0.9160 - val_loss: 0.3355 - val_accuracy: 0.8680\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 9s 875us/step - loss: 0.2080 - accuracy: 0.9173 - val_loss: 0.3240 - val_accuracy: 0.8740\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.2063 - accuracy: 0.9153 - val_loss: 0.3301 - val_accuracy: 0.8730\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 0.2044 - accuracy: 0.9173 - val_loss: 0.3372 - val_accuracy: 0.8690\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 9s 881us/step - loss: 0.2031 - accuracy: 0.9168 - val_loss: 0.3125 - val_accuracy: 0.8760\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 9s 852us/step - loss: 0.1934 - accuracy: 0.9203 - val_loss: 0.4146 - val_accuracy: 0.8720\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 9s 853us/step - loss: 0.1943 - accuracy: 0.9253 - val_loss: 0.3245 - val_accuracy: 0.8780\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 9s 922us/step - loss: 0.1857 - accuracy: 0.9224 - val_loss: 0.3286 - val_accuracy: 0.8800\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 9s 898us/step - loss: 0.1826 - accuracy: 0.9268 - val_loss: 0.3373 - val_accuracy: 0.8750\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 9s 887us/step - loss: 0.1844 - accuracy: 0.9275 - val_loss: 0.3193 - val_accuracy: 0.8800\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 9s 869us/step - loss: 0.1725 - accuracy: 0.9300 - val_loss: 0.3131 - val_accuracy: 0.8800\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 9s 870us/step - loss: 0.1637 - accuracy: 0.9309 - val_loss: 0.2964 - val_accuracy: 0.8860\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 9s 869us/step - loss: 0.1686 - accuracy: 0.9332 - val_loss: 0.3388 - val_accuracy: 0.8770\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 9s 880us/step - loss: 0.1687 - accuracy: 0.9341 - val_loss: 0.3209 - val_accuracy: 0.8810\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.1652 - accuracy: 0.9349 - val_loss: 0.2998 - val_accuracy: 0.8900\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 0.1580 - accuracy: 0.9386 - val_loss: 0.2954 - val_accuracy: 0.8930\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 9s 886us/step - loss: 0.1565 - accuracy: 0.9389 - val_loss: 0.3018 - val_accuracy: 0.8920\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.1556 - accuracy: 0.9404 - val_loss: 0.3104 - val_accuracy: 0.8800\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 9s 869us/step - loss: 0.1543 - accuracy: 0.9401 - val_loss: 0.3067 - val_accuracy: 0.8890\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 9s 867us/step - loss: 0.1473 - accuracy: 0.9439 - val_loss: 0.3318 - val_accuracy: 0.8880\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 9s 881us/step - loss: 0.1493 - accuracy: 0.9400 - val_loss: 0.3286 - val_accuracy: 0.8910\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 9s 875us/step - loss: 0.1530 - accuracy: 0.9420 - val_loss: 0.3268 - val_accuracy: 0.8920\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.1493 - accuracy: 0.9433 - val_loss: 0.2911 - val_accuracy: 0.8940\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 9s 886us/step - loss: 0.1503 - accuracy: 0.9433 - val_loss: 0.2885 - val_accuracy: 0.8910\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 9s 923us/step - loss: 0.1441 - accuracy: 0.9439 - val_loss: 0.3106 - val_accuracy: 0.8880\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 9s 900us/step - loss: 0.1464 - accuracy: 0.9440 - val_loss: 0.3424 - val_accuracy: 0.8800\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 9s 923us/step - loss: 0.1408 - accuracy: 0.9464 - val_loss: 0.2696 - val_accuracy: 0.8950\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 9s 944us/step - loss: 0.1384 - accuracy: 0.9468 - val_loss: 0.3080 - val_accuracy: 0.8920\n",
      "Epoch 109/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 9s 852us/step - loss: 0.1302 - accuracy: 0.9502 - val_loss: 0.3466 - val_accuracy: 0.8970\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 9s 874us/step - loss: 0.1304 - accuracy: 0.9521 - val_loss: 0.3138 - val_accuracy: 0.8930\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 9s 870us/step - loss: 0.1327 - accuracy: 0.9489 - val_loss: 0.3360 - val_accuracy: 0.8850\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 9s 881us/step - loss: 0.1281 - accuracy: 0.9494 - val_loss: 0.3626 - val_accuracy: 0.8860\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 9s 888us/step - loss: 0.1248 - accuracy: 0.9547 - val_loss: 0.3606 - val_accuracy: 0.8860\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 9s 894us/step - loss: 0.1266 - accuracy: 0.9513 - val_loss: 0.3319 - val_accuracy: 0.8880\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.1300 - accuracy: 0.9538 - val_loss: 0.3322 - val_accuracy: 0.8940\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 8s 841us/step - loss: 0.1305 - accuracy: 0.9531 - val_loss: 0.3270 - val_accuracy: 0.8940\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 8s 841us/step - loss: 0.1211 - accuracy: 0.9558 - val_loss: 0.3644 - val_accuracy: 0.8890\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.1197 - accuracy: 0.9532 - val_loss: 0.3499 - val_accuracy: 0.8900\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 9s 864us/step - loss: 0.1174 - accuracy: 0.9557 - val_loss: 0.3586 - val_accuracy: 0.8880\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 9s 862us/step - loss: 0.1210 - accuracy: 0.9552 - val_loss: 0.4017 - val_accuracy: 0.8910\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 9s 856us/step - loss: 0.1188 - accuracy: 0.9557 - val_loss: 0.3842 - val_accuracy: 0.8960\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 8s 844us/step - loss: 0.1119 - accuracy: 0.9594 - val_loss: 0.3817 - val_accuracy: 0.8880\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 8s 844us/step - loss: 0.1213 - accuracy: 0.9552 - val_loss: 0.3609 - val_accuracy: 0.8930\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 8s 844us/step - loss: 0.1149 - accuracy: 0.9580 - val_loss: 0.3603 - val_accuracy: 0.8920\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 9s 858us/step - loss: 0.1234 - accuracy: 0.9544 - val_loss: 0.3424 - val_accuracy: 0.8940\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 9s 875us/step - loss: 0.1116 - accuracy: 0.9576 - val_loss: 0.3532 - val_accuracy: 0.8880\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 9s 883us/step - loss: 0.1044 - accuracy: 0.9617 - val_loss: 0.3448 - val_accuracy: 0.8920\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 9s 853us/step - loss: 0.1128 - accuracy: 0.9582 - val_loss: 0.3989 - val_accuracy: 0.8920\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 9s 867us/step - loss: 0.1077 - accuracy: 0.9592 - val_loss: 0.3526 - val_accuracy: 0.8970\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 8s 847us/step - loss: 0.1080 - accuracy: 0.9609 - val_loss: 0.3536 - val_accuracy: 0.8970\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 8s 845us/step - loss: 0.1065 - accuracy: 0.9620 - val_loss: 0.3820 - val_accuracy: 0.8950\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 9s 850us/step - loss: 0.1021 - accuracy: 0.9607 - val_loss: 0.3784 - val_accuracy: 0.8820\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 9s 853us/step - loss: 0.1005 - accuracy: 0.9633 - val_loss: 0.3873 - val_accuracy: 0.8930\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 9s 852us/step - loss: 0.1043 - accuracy: 0.9650 - val_loss: 0.3916 - val_accuracy: 0.8920\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 9s 870us/step - loss: 0.0995 - accuracy: 0.9649 - val_loss: 0.3916 - val_accuracy: 0.8980\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 9s 895us/step - loss: 0.0978 - accuracy: 0.9649 - val_loss: 0.4920 - val_accuracy: 0.8890\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 9s 870us/step - loss: 0.1001 - accuracy: 0.9661 - val_loss: 0.3889 - val_accuracy: 0.8910\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 9s 909us/step - loss: 0.1007 - accuracy: 0.9648 - val_loss: 0.4445 - val_accuracy: 0.8910\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 9s 903us/step - loss: 0.0940 - accuracy: 0.9651 - val_loss: 0.3573 - val_accuracy: 0.8950\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 0.0911 - accuracy: 0.9651 - val_loss: 0.4084 - val_accuracy: 0.8920\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 9s 881us/step - loss: 0.0939 - accuracy: 0.9666 - val_loss: 0.3909 - val_accuracy: 0.8820\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 9s 850us/step - loss: 0.0906 - accuracy: 0.9670 - val_loss: 0.4606 - val_accuracy: 0.8850\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 9s 864us/step - loss: 0.0879 - accuracy: 0.9702 - val_loss: 0.4142 - val_accuracy: 0.8930\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.0889 - accuracy: 0.9700 - val_loss: 0.4453 - val_accuracy: 0.8850\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.0871 - accuracy: 0.9684 - val_loss: 0.4799 - val_accuracy: 0.8880\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.0926 - accuracy: 0.9676 - val_loss: 0.4087 - val_accuracy: 0.8910\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 9s 853us/step - loss: 0.0816 - accuracy: 0.9699 - val_loss: 0.4295 - val_accuracy: 0.8950\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.0883 - accuracy: 0.9716 - val_loss: 0.4286 - val_accuracy: 0.8940\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 9s 850us/step - loss: 0.0847 - accuracy: 0.9693 - val_loss: 0.4191 - val_accuracy: 0.8940\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.0800 - accuracy: 0.9705 - val_loss: 0.4605 - val_accuracy: 0.8930\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 8s 845us/step - loss: 0.0888 - accuracy: 0.9708 - val_loss: 0.4520 - val_accuracy: 0.8960\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 8s 839us/step - loss: 0.0810 - accuracy: 0.9714 - val_loss: 0.4589 - val_accuracy: 0.8960\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.0808 - accuracy: 0.9723 - val_loss: 0.4412 - val_accuracy: 0.8950\n",
      "Epoch 154/200\n",
      "10000/10000 [==============================] - 8s 848us/step - loss: 0.0686 - accuracy: 0.9746 - val_loss: 0.5422 - val_accuracy: 0.8850\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 8s 841us/step - loss: 0.0742 - accuracy: 0.9736 - val_loss: 0.5833 - val_accuracy: 0.8700\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 9s 853us/step - loss: 0.0754 - accuracy: 0.9730 - val_loss: 0.4701 - val_accuracy: 0.8920\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 8s 845us/step - loss: 0.0751 - accuracy: 0.9736 - val_loss: 0.4828 - val_accuracy: 0.8870\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 8s 848us/step - loss: 0.0755 - accuracy: 0.9712 - val_loss: 0.4786 - val_accuracy: 0.8930\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.0708 - accuracy: 0.9764 - val_loss: 0.4409 - val_accuracy: 0.8970\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 9s 850us/step - loss: 0.0739 - accuracy: 0.9750 - val_loss: 0.4985 - val_accuracy: 0.8980\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 8s 845us/step - loss: 0.0694 - accuracy: 0.9739 - val_loss: 0.4816 - val_accuracy: 0.8920\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0726 - accuracy: 0.9746 - val_loss: 0.4856 - val_accuracy: 0.8920\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.0721 - accuracy: 0.9742 - val_loss: 0.4524 - val_accuracy: 0.8960\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0662 - accuracy: 0.9770 - val_loss: 0.4535 - val_accuracy: 0.8890\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0681 - accuracy: 0.9788 - val_loss: 0.4626 - val_accuracy: 0.8930\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0611 - accuracy: 0.9785 - val_loss: 0.5481 - val_accuracy: 0.8970\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0693 - accuracy: 0.9772 - val_loss: 0.5296 - val_accuracy: 0.8840\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0662 - accuracy: 0.9776 - val_loss: 0.4833 - val_accuracy: 0.8940\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0604 - accuracy: 0.9783 - val_loss: 0.5058 - val_accuracy: 0.8920\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0622 - accuracy: 0.9791 - val_loss: 0.4658 - val_accuracy: 0.8960\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0655 - accuracy: 0.9776 - val_loss: 0.4873 - val_accuracy: 0.8980\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0606 - accuracy: 0.9795 - val_loss: 0.5296 - val_accuracy: 0.8940\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0651 - accuracy: 0.9775 - val_loss: 0.5058 - val_accuracy: 0.8920\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0538 - accuracy: 0.9817 - val_loss: 0.5015 - val_accuracy: 0.8960\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.0535 - accuracy: 0.9820 - val_loss: 0.5744 - val_accuracy: 0.8920\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0578 - accuracy: 0.9795 - val_loss: 0.5412 - val_accuracy: 0.8970\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0628 - accuracy: 0.9818 - val_loss: 0.5946 - val_accuracy: 0.8850\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0606 - accuracy: 0.9806 - val_loss: 0.5356 - val_accuracy: 0.8900\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0630 - accuracy: 0.9772 - val_loss: 0.5876 - val_accuracy: 0.8860\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0650 - accuracy: 0.9789 - val_loss: 0.5317 - val_accuracy: 0.8920\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.5428 - val_accuracy: 0.8940\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0593 - accuracy: 0.9814 - val_loss: 0.4879 - val_accuracy: 0.9030\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.0546 - accuracy: 0.9825 - val_loss: 0.5683 - val_accuracy: 0.8950\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.0511 - accuracy: 0.9820 - val_loss: 0.5930 - val_accuracy: 0.8800\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.5756 - val_accuracy: 0.8990\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 15s 1ms/step - loss: 0.0544 - accuracy: 0.9824 - val_loss: 0.5975 - val_accuracy: 0.8830\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.0545 - accuracy: 0.9812 - val_loss: 0.6070 - val_accuracy: 0.8870\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0502 - accuracy: 0.9838 - val_loss: 0.6020 - val_accuracy: 0.8890\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0495 - accuracy: 0.9844 - val_loss: 0.5459 - val_accuracy: 0.8910\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0474 - accuracy: 0.9845 - val_loss: 0.6584 - val_accuracy: 0.8860\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0548 - accuracy: 0.9824 - val_loss: 0.5953 - val_accuracy: 0.8880\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0472 - accuracy: 0.9851 - val_loss: 0.5344 - val_accuracy: 0.9030\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.0525 - accuracy: 0.9834 - val_loss: 0.6044 - val_accuracy: 0.8900\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0477 - accuracy: 0.9840 - val_loss: 0.6251 - val_accuracy: 0.8840\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0452 - accuracy: 0.9852 - val_loss: 0.6477 - val_accuracy: 0.8880\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0496 - accuracy: 0.9845 - val_loss: 0.5473 - val_accuracy: 0.9000\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0410 - accuracy: 0.9862 - val_loss: 0.6228 - val_accuracy: 0.8910\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0501 - accuracy: 0.9856 - val_loss: 0.6428 - val_accuracy: 0.8940\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0423 - accuracy: 0.9843 - val_loss: 0.6232 - val_accuracy: 0.8930\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.0416 - accuracy: 0.9870 - val_loss: 0.5987 - val_accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_inputs, train_queries], train_answers, \n",
    "                    batch_size = 32, epochs = 200,\n",
    "                    validation_data=([test_inputs, test_queries], test_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "#### Plotting out the Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([test_inputs, test_queries]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary got the milk there . John moved to the bedroom .\n"
     ]
    }
   ],
   "source": [
    "story =' '.join(word for word in test_data[0][0])\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is John in the kitchen ?\n"
     ]
    }
   ],
   "source": [
    "query = ' '.join(word for word in test_data[0][1])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Test Answer from Data is: no\n"
     ]
    }
   ],
   "source": [
    "print(\"True Test Answer from Data is:\",test_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  0.9951676\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's try testing with our own story*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story = \"John left the kitchen . Sandra dropped the football in the garden .\"\n",
    "my_question = \"Is the football in the kitchen ?\"\n",
    "mydata = [(my_story.split(),my_question.split(),'no')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story,my_ques,my_ans = vectorize_data(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([ my_story, my_ques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  0.9951676\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
